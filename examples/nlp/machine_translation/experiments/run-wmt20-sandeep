#!/bin/bash
INSTANCE=dgx1v.32g.8.norm.beta
PROJECT=en_es_wmt20
DATAID=73242
STEPS=300000
WANDBLOGIN=a380d6dd9912914b6b85dbd396f528c5fc465766
VOCAB_SIZE=32000
set -e
EXPNAME=EsEn-v$VOCAB_SIZE-lr0.0004-wr0.1ngc-tarred
ngc batch run --name "$EXPNAME" --preempt RUNONCE \
    --image "nvcr.io/nvidia/pytorch:21.03-py3" \
    --ace nv-us-west-2 \
    --instance $INSTANCE \
    --commandline "nvidia-smi && apt-get update && apt-get install -y libsndfile1 ffmpeg && \
    pip install --upgrade wandb && pip install Cython && wandb login $WANDBLOGIN && \
    git clone https://github.com/NVIDIA/NeMo.git && cd NeMo && \
    ./reinstall.sh && \
    cp -R /data/parallel/sharded_tarfiles_60_even/ /raid/ && \
    cp -R /data/parallel/newstest2012-es-en.clean.tok.* /raid/ && \
    cp -R /data/parallel/newstest2013-es-en.clean.tok.* /raid/ && \
    python examples/nlp/machine_translation/enc_dec_nmt.py -cn aayn_base \
        trainer.gpus=8 \
        ~trainer.max_epochs \
        +trainer.max_steps=$STEPS \
        +trainer.val_check_interval=1000 \
        +exp_manager.exp_dir=/results/wmt20_es_en_deepenc \
        +exp_manager.create_wandb_logger=True \
        +exp_manager.wandb_logger_kwargs.name=$EXPNAME \
        +exp_manager.wandb_logger_kwargs.project=$PROJECT \
        +exp_manager.resume_if_exists=True \
        +exp_manager.resume_ignore_no_checkpoint=True \
        +exp_manager.create_checkpoint_callback=True \
        +exp_manager.checkpoint_callback_params.monitor=val_sacreBLEU \
        +exp_manager.checkpoint_callback_params.mode=max \
        model.encoder.hidden_size=1024 \
        model.encoder.inner_size=4096 \
        model.encoder.num_attention_heads=16 \
        model.encoder.attn_layer_dropout=0.1 \
        model.encoder.ffn_dropout=0.1 \
        model.encoder.num_layers=12 \
        model.decoder.hidden_size=1024 \
        model.decoder.inner_size=4096 \
        model.decoder.num_attention_heads=16 \
        model.decoder.attn_layer_dropout=0.1 \
        model.decoder.ffn_dropout=0.1 \
        model.decoder.num_layers=2 \
        model.optim.lr=0.0004 \
        model.encoder_tokenizer.tokenizer_model=/raid/sharded_tarfiles_60_even/tokenizer.32000.BPE.model \
        model.decoder_tokenizer.tokenizer_model=/raid/sharded_tarfiles_60_even/tokenizer.32000.BPE.model \
        model.encoder_tokenizer.bpe_dropout=0.1 \
        model.decoder_tokenizer.bpe_dropout=0.1 \
        model.train_ds.src_file_name=/raid/sharded_tarfiles_60_even/batches.tokens.16000._OP_1..302_CL_.tar \
        model.train_ds.tgt_file_name=/raid/sharded_tarfiles_60_even/batches.tokens.16000._OP_1..302_CL_.tar \
        model.train_ds.tokens_in_batch=16000 \
        model.train_ds.use_tarred_dataset=true \
        model.train_ds.metadata_path=/raid/sharded_tarfiles_60_even/metadata.json \
        model.validation_ds.src_file_name=/raid/newstest2012-es-en.clean.tok.src \
        model.validation_ds.tgt_file_name=/raid/newstest2012-es-en.clean.tok.ref \
        model.test_ds.src_file_name=/raid/newstest2013-es-en.clean.tok.src \
        model.test_ds.tgt_file_name=/raid/newstest2013-es-en.clean.tok.ref" \
    --result /results/ \
    --org nvidian \
    --team swdl-ai-apps \
    --datasetid $DATAID:/data/


###########################################
#!/bin/bash
INSTANCE=dgx1v.32g.8.norm.beta
PROJECT=de_en_wmt21
DATAID=<INSERT YOUR ID HERE>
STEPS=300000
WANDBLOGIN=a380d6dd9912914b6b85dbd396f528c5fc465766
VOCAB_SIZE=32000
set -e
EXPNAME=DeEn-v$VOCAB_SIZE-lr0.0004-wr0.1ngc-tarred-en-de
ngc batch run --name "$EXPNAME" --preempt RUNONCE \
    --image "nvcr.io/nvidia/pytorch:21.03-py3" \
    --ace nv-us-west-2 \
    --instance $INSTANCE \
    --commandline "nvidia-smi && apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y libsndfile1 ffmpeg && \
    pip install wandb==0.10.21 && pip install Cython && wandb login $WANDBLOGIN && \
    git clone https://github.com/NVIDIA/NeMo.git && cd NeMo && pip uninstall -y torchtext && ./reinstall.sh && \
    cp -R /data/wmt21_de_en_tarred_r2l_16ktokens /raid/ && \
    cp -R /data/newstest2014-de-en.clean.tok.* /raid/ && \
    python examples/nlp/machine_translation/enc_dec_nmt.py -cn aayn_base \
        trainer.gpus=8 \
        ~trainer.max_epochs \
        +trainer.max_steps=300000 \
        +trainer.val_check_interval=1000 \
        +exp_manager.exp_dir=/results/wmt20_de_en_6x6 \
        +exp_manager.create_wandb_logger=True \
        +exp_manager.wandb_logger_kwargs.name=DeEn-v32000-r2l-6x6-ngctarred \
        +exp_manager.wandb_logger_kwargs.project=de_en_wmt21 \
        +exp_manager.resume_if_exists=True \
        +exp_manager.resume_ignore_no_checkpoint=True \
        +exp_manager.create_checkpoint_callback=True \
        +exp_manager.checkpoint_callback_params.monitor=val_sacreBLEU \
        +exp_manager.checkpoint_callback_params.mode=max \
        +exp_manager.checkpoint_callback_params.save_top_k=5 \
        model.src_language=de \
        model.tgt_language=en \
        model.encoder.hidden_size=1024 \
        model.encoder.inner_size=4096 \
        model.encoder.num_attention_heads=16 \
        model.encoder.attn_layer_dropout=0.1 \
        model.encoder.ffn_dropout=0.1 \
        model.encoder.num_layers=6 \
        model.encoder.pre_ln=false \
        model.decoder.hidden_size=1024 \
        model.decoder.inner_size=4096 \
        model.decoder.num_attention_heads=16 \
        model.decoder.attn_layer_dropout=0.1 \
        model.decoder.ffn_dropout=0.1 \
        model.decoder.num_layers=6 \
        model.decoder.pre_ln=false \
        model.optim.lr=0.0004 \
        model.encoder_tokenizer.tokenizer_model=/raid/wmt21_de_en_tarred_r2l_16ktokens/shared_tokenizer.32000.BPE.model \
        model.decoder_tokenizer.tokenizer_model=/raid/wmt21_de_en_tarred_r2l_16ktokens/shared_tokenizer.32000.BPE.model \
        model.train_ds.tar_files=/raid/wmt21_de_en_tarred_r2l_16ktokens/parallel.batches.tokens.16000._OP_1..229_CL_.tar \
        model.train_ds.metadata_file=/raid/wmt21_de_en_tarred_r2l_16ktokens/metadata.tokens.16000.json \
        model.train_ds.tokens_in_batch=16000 \
        model.train_ds.use_tarred_dataset=true \
        model.validation_ds.src_file_name=/raid/newstest2014-de-en.clean.tok.src \
        model.validation_ds.tgt_file_name=/raid/newstest2014-de-en.clean.tok.ref \
        model.test_ds.src_file_name=/raid/newstest2014-de-en.clean.tok.src \
        model.test_ds.tgt_file_name=/raid/newstest2014-de-en.clean.tok.ref" \
        --result /results/ \
        --org nvidian \
        --team swdl-ai-apps \
        --datasetid $DATAID:/data/