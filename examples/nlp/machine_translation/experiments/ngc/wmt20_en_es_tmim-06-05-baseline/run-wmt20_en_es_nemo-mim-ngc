#!/usr/bin/env bash

script_name=`basename $0`
echo "${script_name} <PROJECT_TAG> <EXP_TAG> <ATT_BRIDGE_K> <ATT_BRIDGE_SIZE> <MODEL_TYPE> <RECON> <LABEL_SMOOTHING> <LEN_PENALTY> <GPU_NUM>"

PROJECT_TAG=${1:--06-05-baseline}
EXP_TAG=${2:-}
ATT_BRIDGE_K=${3:-32}
ATT_BRIDGE_SIZE=${4:-2048}
MODEL_TYPE=${5:-mim}
RECON=${6:-true}
LABEL_SMOOTHING=${7:-0.0}
LEN_PENALTY=${8:-0.0}
GPU_NUM=${9:-8}

INSTANCE=dgx1v.32g.${GPU_NUM}.norm.beta
NGC_CONTAINER='nvcr.io/nvidia/pytorch:21.03-py3'

PROJECT_NAME=wmt20_en_es_tmim${PROJECT_TAG}
EXP_NAME="${MODEL_TYPE}_k${ATT_BRIDGE_K}x${ATT_BRIDGE_SIZE}_recon-${RECON}_brsz${ATT_BRIDGE_SIZE}_ls${LABEL_SMOOTHING}_lp${LEN_PENALTY}${EXP_TAG}"
EXP_NAME_NGC="${PROJECT_NAME}-${EXP_NAME} ml-model.aayn-tmim"


WANDBLOGIN=1dadc548dae732414fcb97918f25942325315029

STEPS=300000
VOCAB_SIZE=32000
DATA_ID=73242

# && pip install -U omegaconf==2.0.6 \
# && pip install -U hydra==1.0.4 \

# command

read -d '' cmd <<EOF
nvidia-smi \
&& export DEBIAN_FRONTEND=noninteractive \
&& apt-get update \
&& apt-get install -y libsndfile1 ffmpeg \
&& pip install --upgrade wandb \
&& pip install Cython \
&& wandb login ${WANDBLOGIN} \
&& git clone https://github.com/michalivne/NeMo.git \
&& cd NeMo/ \
&& git checkout nmt-translationMIM \
&& ./reinstall.sh \
&& cd examples/nlp/machine_translation \
&& cp -R /data/parallel/sharded_tarfiles_60_even/ /raid/ \
&& cp -R /data/parallel/newstest2012-es-en.clean.tok.* /raid/ \
&& cp -R /data/parallel/newstest2013-es-en.clean.tok.* /raid/ \
&& python -- enc_dec_nmt-mim.py \
      --config-path=conf \
      --config-name=mim_base \
      'trainer.gpus=-1' \
      '~trainer.max_epochs' \
      exp_manager.name=${EXP_NAME} \
      +trainer.max_steps=${STEPS} \
      +trainer.val_check_interval=1000 \
      +exp_manager.exp_dir=/results/${PROJECT_NAME} \
      +exp_manager.create_wandb_logger=True \
      +exp_manager.wandb_logger_kwargs.name=${EXP_NAME} \
      +exp_manager.wandb_logger_kwargs.project=${PROJECT_NAME} \
      +exp_manager.resume_if_exists=True \
      +exp_manager.resume_ignore_no_checkpoint=True \
      +exp_manager.create_checkpoint_callback=True \
      +exp_manager.checkpoint_callback_params.monitor=val_sacreBLEU \
      +exp_manager.checkpoint_callback_params.mode=max \
      +exp_manager.checkpoint_callback_params.save_top_k=1 \
      +exp_manager.checkpoint_callback_params.always_save_nemo=true \
      +exp_manager.checkpoint_callback_params.save_best_model=true \
      model.beam_size=4 \
      model.max_generation_delta=200 \
      model.len_pen=${LEN_PENALTY} \
      model.label_smoothing=${LABEL_SMOOTHING} \
      model.src_language=es \
      model.tgt_language=en \
      model.model_type=${MODEL_TYPE} \
      model.latent_size=1024 \
      model.ortho_loss_coef=0.0 \
      model.att_bridge_k=${ATT_BRIDGE_K} \
      model.att_bridge_size=${ATT_BRIDGE_SIZE} \
      model.recon_per_token=${RECON} \
      model.encoder.hidden_size=1024 \
      model.encoder.inner_size=4096 \
      model.encoder.num_attention_heads=16 \
      model.encoder.attn_layer_dropout=0.1 \
      model.encoder.ffn_dropout=0.1 \
      model.encoder.num_layers=6 \
      model.decoder.hidden_size=1024 \
      model.decoder.inner_size=4096 \
      model.decoder.num_attention_heads=16 \
      model.decoder.attn_layer_dropout=0.1 \
      model.decoder.ffn_dropout=0.1 \
      model.decoder.num_layers=6 \
      model.optim.lr=0.0004 \
      model.encoder_tokenizer.library=yttm \
      model.decoder_tokenizer.library=yttm \
      model.encoder_tokenizer.tokenizer_model=/raid/sharded_tarfiles_60_even/tokenizer.32000.BPE.model \
      model.decoder_tokenizer.tokenizer_model=/raid/sharded_tarfiles_60_even/tokenizer.32000.BPE.model \
      model.train_ds.tokens_in_batch=16000 \
      model.train_ds.use_tarred_dataset=true \
      +model.train_ds.reverse_lang_direction=false \
      model.train_ds.tar_files=/raid/sharded_tarfiles_60_even/batches.tokens.16000._OP_1..302_CL_.tar \
      model.train_ds.metadata_file=/raid/sharded_tarfiles_60_even/metadata.json \
      model.train_ds.src_file_name=/data/parallel/train.clean.en-es.60.tok.en.shuffled \
      model.train_ds.tgt_file_name=/data/parallel/train.clean.en-es.60.tok.es.shuffled \
      model.preproc_out_dir=/raid/sharded_tarfiles_60_even/ \
      model.validation_ds.src_file_name=/raid/newstest2012-es-en.clean.tok.ref \
      model.validation_ds.tgt_file_name=/raid/newstest2012-es-en.clean.tok.src \
      model.test_ds.src_file_name=/raid/newstest2013-es-en.clean.tok.ref \
      model.test_ds.tgt_file_name=/raid/newstest2013-es-en.clean.tok.src \
      do_training=true
EOF

# launch script
set -e

ngc batch run \
  --name "${EXP_NAME_NGC}" \
  --preempt RUNONCE \
  --image "${NGC_CONTAINER}" \
  --ace nv-us-west-2 \
  --instance "${INSTANCE}" \
  --org nvidian \
  --team swdl-ai-apps \
  --result /results/ \
  --datasetid ${DATA_ID}:/data/ \
  --commandline "${cmd}"
