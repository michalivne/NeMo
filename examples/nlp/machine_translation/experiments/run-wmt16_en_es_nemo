#!/usr/bin/env bash

script_name=`basename $0`
echo "${script_name} <SentenceMIM API key> <ENC_TOKENIZER> <DEC_TOKENIZER> <ENC_MODEL> <DEC_MODEL> <GPU_NUM>"

GITHUB_KEY=$1
ENC_TOKENIZER=${2:-yttm}
DEC_TOKENIZER=${3:-yttm}
ENC_MODEL=${4:-/data/tokenizer/tokenizer.BPE.68957.8192.model}
DEC_MODEL=${5:-/data/tokenizer/tokenizer.BPE.68957.8192.model}
GPU_NUM=${6:-2}

INSTANCE=dgx1v.32g.${GPU_NUM}.norm.beta
NGC_CONTAINER='nvcr.io/nvidia/pytorch:20.11-py3'
EXP_NAME=EsEn-wmt16_${ENC_TOKENIZER}-${DEC_TOKENIZER}
EXP_NAME_NGC="${EXP_NAME} ml-model.aayn-emim"



# /data/tokenizer/tokenizer.BPE.68957.8192.model
# /data/smim/smim.1796586.model

# ngc dataset upload --source wmt16_en_es_nemo --desc "WMT16 eng-esp" wmt16_en_es_nemo
# Dataset ID: 68957
# ngc dataset upload --source wmt16_en_es_nemo-preproc --desc "WMT16 eng-esp preproc" wmt16_en_es_nemo-preproc
# Dataset ID: 76378

#  ngc dataset upload --source tokenizer.BPE.68957.8192.model --desc "WMT16 eng-esp bpe tokeizer" tokenizer.BPE.68957.8192.model
# Dataset ID: 76234
# ngc dataset upload --source smim.1796586.model --desc "smim.1796586.model" smim.1796586.model
# Dataset ID: 76327

# command

read -d '' cmd <<EOF
nvidia-smi \
&& apt-get update \
&& apt-get install -y libsndfile1 ffmpeg \
&& pip install Cython \
&& git clone https://michalivne:${GITHUB_KEY}@github.com/seraphlabs-ca/SentenceMIM.git \
&& pushd SentenceMIM \
&& pip install -r requirements.txt \
&& popd \
&& git clone https://github.com/michalivne/NeMo.git \
&& cd NeMo/ \
&& git checkout nmt_embeddingMIM \
&& ./reinstall.sh \
&& cd examples/nlp/machine_translation \
&& python enc_dec_nmt.py \
      --config-path=conf \
      --config-name=emim_base \
      exp_manager.name=${ENC_TOKENIZER}-${DEC_TOKENIZER} \
      'trainer.gpus=-1' \
      '~trainer.max_epochs' \
      +trainer.max_steps=150000 \
      +exp_manager.exp_dir=/results/${ENC_TOKENIZER}-${DEC_TOKENIZER} \
      model.beam_size=4 \
      model.max_generation_delta=5 \
      model.label_smoothing=0.1 \
      model.encoder_tokenizer.tokenizer_name=${ENC_TOKENIZER} \
      model.decoder_tokenizer.tokenizer_name=${DEC_TOKENIZER} \
      model.encoder_tokenizer.tokenizer_model=${ENC_MODEL}  \
      model.decoder_tokenizer.tokenizer_model=${DEC_MODEL}  \
      model.encoder.num_layers=6 \
      model.encoder.hidden_size=512 \
      model.encoder.inner_size=2048 \
      model.encoder.num_attention_heads=8 \
      model.encoder.ffn_dropout=0.1 \
      model.decoder.num_layers=6 \
      model.decoder.hidden_size=512 \
      model.decoder.inner_size=2048 \
      model.decoder.num_attention_heads=8 \
      model.decoder.ffn_dropout=0.1 \
      model.train_ds.src_file_name=/data/wmt16_en_es_nemo/train.clean.es.shuffled \
      model.train_ds.tgt_file_name=/data/wmt16_en_es_nemo/train.clean.en.shuffled \
      model.train_ds.tokens_in_batch=12500 \
      model.validation_ds.src_file_name=/data/wmt16_en_es_nemo/wmt13-en-es.ref \
      model.validation_ds.tgt_file_name=/data/wmt16_en_es_nemo/wmt13-en-es.src \
      model.validation_ds.tokens_in_batch=8192 \
      model.test_ds.src_file_name=/data/wmt16_en_es_nemo/wmt13-en-es.ref \
      model.test_ds.tgt_file_name=/data/wmt16_en_es_nemo/wmt13-en-es.src \
      model.optim.lr=0.001  \
      model.optim.sched.warmup_ratio=0.05 \
      model.train_ds.use_cache=false \
      trainer.amp_level=O0 \
      trainer.precision=32 \
      model.train_ds.use_tarred_dataset=true \
      model.preproc_out_dir=/data/wmt16_en_es_nemo-preproc
EOF

# launch script
# --datasetid 76378:/data/wmt16_en_es_nemo-preproc/ \
set -e

ngc batch run \
  --name "${EXP_NAME}" \
  --preempt RUNONCE \
  --image "${NGC_CONTAINER}" \
  --ace nv-us-west-2 \
  --instance "${INSTANCE}" \
  --org nvidian \
  --result /results/ \
  --datasetid 68957:/data/wmt16_en_es_nemo/ \
  --datasetid 76234:/data/tokenizer/ \
  --datasetid 76327:/data/smim/ \
  --commandline "${cmd}"

